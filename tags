!_TAG_FILE_FORMAT	2	/extended format; --format=1 will not append ;" to lines/
!_TAG_FILE_SORTED	1	/0=unsorted, 1=sorted, 2=foldcase/
!_TAG_PROGRAM_AUTHOR	Darren Hiebert	/dhiebert@users.sourceforge.net/
!_TAG_PROGRAM_NAME	Exuberant Ctags	//
!_TAG_PROGRAM_URL	http://ctags.sourceforge.net	/official site/
!_TAG_PROGRAM_VERSION	5.9~svn20110310	//
C	run.py	/^C = 5$/;"	v
SAVE_PARAMS_EVERY	sgd.py	/^SAVE_PARAMS_EVERY = 5000$/;"	v
StanfordSentiment	utils/treebank.py	/^class StanfordSentiment:$/;"	c
__init__	utils/treebank.py	/^    def __init__(self, path=None, tablesize = 1000000):$/;"	m	class:StanfordSentiment
allSentences	utils/treebank.py	/^    def allSentences(self):$/;"	m	class:StanfordSentiment
axis	run.py	/^    axis=0)$/;"	v
bbox	run.py	/^        bbox=dict(facecolor='green', alpha=0.1))$/;"	v
categorify	utils/treebank.py	/^    def categorify(self, label):$/;"	m	class:StanfordSentiment
coord	run.py	/^coord = temp.dot(U[:,0:2])$/;"	v
covariance	run.py	/^covariance = 1.0 \/ len(visualizeIdx) * temp.T.dot(temp)$/;"	v
dataset	run.py	/^dataset = StanfordSentiment()$/;"	v
dataset_split	utils/treebank.py	/^    def dataset_split(self):$/;"	m	class:StanfordSentiment
dimVectors	run.py	/^dimVectors = 10$/;"	v
dummySampleTokenIdx	word2vec.py	/^    def dummySampleTokenIdx():$/;"	f	function:test_word2vec
getDevSentences	utils/treebank.py	/^    def getDevSentences(self):$/;"	m	class:StanfordSentiment
getNegativeSamples	word2vec.py	/^def getNegativeSamples(outsideWordIdx, dataset, K):$/;"	f
getRandomContext	utils/treebank.py	/^    def getRandomContext(self, C=5):$/;"	m	class:StanfordSentiment
getRandomContext	word2vec.py	/^    def getRandomContext(C):$/;"	f	function:test_word2vec
getRandomTrainSentence	utils/treebank.py	/^    def getRandomTrainSentence(self):$/;"	m	class:StanfordSentiment
getSplitSentences	utils/treebank.py	/^    def getSplitSentences(self, split=0):$/;"	m	class:StanfordSentiment
getTestSentences	utils/treebank.py	/^    def getTestSentences(self):$/;"	m	class:StanfordSentiment
getTrainSentences	utils/treebank.py	/^    def getTrainSentences(self):$/;"	m	class:StanfordSentiment
gradOutsideVecs	word2vec.py	/^    gradOutsideVecs = -1*(1-simoid(np.dot(outsideVectors[outsideWordIdx],centerWordVec)))*centerWordVec.T$/;"	v
grad_tests_negsamp	utils/gradcheck.py	/^def grad_tests_negsamp(skipgram, dummy_tokens, dummy_vectors, dataset, negSamplingLossAndGradient):$/;"	f
grad_tests_softmax	utils/gradcheck.py	/^def grad_tests_softmax(skipgram, dummy_tokens, dummy_vectors, dataset):$/;"	f
gradcheck_naive	utils/gradcheck.py	/^def gradcheck_naive(f, x, gradientText):$/;"	f
grandCenterVec	word2vec.py	/^    grandCenterVec = np.zeros(centerWordVec.shape)$/;"	v
indices	word2vec.py	/^    indices = [outsideWordIdx] + negSampleWordIndices$/;"	v
load_saved_params	sgd.py	/^def load_saved_params():$/;"	f
loss	word2vec.py	/^    loss = 0.0$/;"	v
nWords	run.py	/^nWords = len(tokens)$/;"	v
naiveSoftmaxLossAndGradient	word2vec.py	/^def naiveSoftmaxLossAndGradient($/;"	f
negSampleWordIndices	word2vec.py	/^    negSampleWordIndices = getNegativeSamples(outsideWordIdx, dataset, K)$/;"	v
negSamplingLossAndGradient	word2vec.py	/^def negSamplingLossAndGradient($/;"	f
normalizeRows	utils/utils.py	/^def normalizeRows(x):$/;"	f
numSentences	utils/treebank.py	/^    def numSentences(self):$/;"	m	class:StanfordSentiment
rejectProb	utils/treebank.py	/^    def rejectProb(self):$/;"	m	class:StanfordSentiment
sampleTable	utils/treebank.py	/^    def sampleTable(self):$/;"	m	class:StanfordSentiment
sampleTokenIdx	utils/treebank.py	/^    def sampleTokenIdx(self):$/;"	m	class:StanfordSentiment
sanity_check	sgd.py	/^def sanity_check():$/;"	f
save_params	sgd.py	/^def save_params(iter, params):$/;"	f
sent_labels	utils/treebank.py	/^    def sent_labels(self):$/;"	m	class:StanfordSentiment
sentences	utils/treebank.py	/^    def sentences(self):$/;"	m	class:StanfordSentiment
sgd	sgd.py	/^def sgd(f, x0, step, iterations, postprocessing=None, useSaved=False,$/;"	f
sigmoid	word2vec.py	/^def sigmoid(x):$/;"	f
skipgram	word2vec.py	/^def skipgram(currentCenterWord, windowSize, outsideWords, word2Ind,$/;"	f
softmax	utils/utils.py	/^def softmax(x):$/;"	f
startTime	run.py	/^startTime=time.time()$/;"	v
temp	run.py	/^temp = (visualizeVecs - np.mean(visualizeVecs, axis=0))$/;"	v
test_word2vec	word2vec.py	/^def test_word2vec():$/;"	f
tokens	run.py	/^tokens = dataset.tokens()$/;"	v
tokens	utils/treebank.py	/^    def tokens(self):$/;"	m	class:StanfordSentiment
visualizeIdx	run.py	/^visualizeIdx = [tokens[word] for word in visualizeWords]$/;"	v
visualizeVecs	run.py	/^visualizeVecs = wordVectors[visualizeIdx, :]$/;"	v
visualizeWords	run.py	/^visualizeWords = [$/;"	v
word2vec_sgd_wrapper	word2vec.py	/^def word2vec_sgd_wrapper(word2vecModel, word2Ind, wordVectors, dataset, $/;"	f
wordVectors	run.py	/^wordVectors = np.concatenate($/;"	v
wordVectors	run.py	/^wordVectors = sgd($/;"	v
